{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7514b796",
   "metadata": {},
   "source": [
    "## Grouped Query Attention From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8d31d",
   "metadata": {},
   "source": [
    "In GQA the Query heads are divide into groups, where each group share a single Key & Value head.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c7540",
   "metadata": {},
   "source": [
    "**How it Works**\n",
    "- **Grouping** Queries are partitioned into *G* groups. Each group shares the same computed Keys and Values.\n",
    "- **Shared Computation** For each group, the Key and Value projections are shared, reducing the number of key/value computations and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd53b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ac1ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MHSA(nn.Module):\n",
    "    def __init__(self, D_model = 512, head_dim = 64, causal = True):\n",
    "        super().__init__()\n",
    "        self.D_model = D_model\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "        assert D_model % head_dim ==0, \"Dimension of model should be a multiple of head_dim\"\n",
    "        self.num_heads = D_model // head_dim\n",
    "        self.causal = causal\n",
    "        \n",
    "        self.wq = nn.Linear(D_model, D_model)\n",
    "        self.wk = nn.Linear(D_model, D_model)\n",
    "        self.wv = nn.Linear(D_model, D_model)\n",
    "        self.wo = nn.Linear(D_model, D_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, S, D = x.shape\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        \n",
    "        q = q.reshape(B, self.num_heads, S, self.head_dim)\n",
    "        k = k.reshape(B, self.num_heads, S, self.head_dim)\n",
    "        v = v.reshape(B, self.num_heads, S, self.head_dim)\n",
    "        \n",
    "        denominator = torch.sqrt(torch.tensor(self.head_dim))\n",
    "        attn_weight = torch.einsum('bnij, bnkj->bnik', q, k)\n",
    "        attn_weight = attn_weight / denominator\n",
    "        \n",
    "        attn_score = F.softmax(attn_weight, dim = -1)\n",
    "        \n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones_like(attn_score), diagonal=1).bool()\n",
    "            attn_score = attn_score.masked_fill(mask, float('-inf'))\n",
    "            \n",
    "        out = torch.einsum('bnik,bnjd->bnid', attn_score, v)\n",
    "        out = out.reshape(B, S, -1)\n",
    "        return self.wo(out)\n",
    "\n",
    "B, S, D = 8 ,512, 768\n",
    "attn = MHSA(D_model=D)\n",
    "x = torch.randn(B, S, D)\n",
    "attn(x).shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79edce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "    def __init__(self, D_model = 512, head_dim = 64, causal = True, group_size = 4):\n",
    "        super().__init__()\n",
    "        self.D_model = D_model\n",
    "        self.head_dim = head_dim\n",
    "        self.causal = causal\n",
    "        self.group_size = group_size\n",
    "        \n",
    "        assert D_model % head_dim == 0, \"Error!!!!!!\"\n",
    "        self.num_query_heads = D_model // head_dim\n",
    "        self.num_kv_heads = self.num_query_heads // self.group_size\n",
    "        self.causal = causal\n",
    "        \n",
    "        self.D_kv = self.num_kv_heads * self.head_dim\n",
    "        \n",
    "        self.wq = nn.Linear(D_model, D_model)\n",
    "        self.wk = nn.Linear(D_model, self.D_kv)\n",
    "        self.wv = nn.Linear(D_model, self.D_kv)\n",
    "        self.wo = nn.Linear(D_model, D_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, S, D = x.shape\n",
    "        \n",
    "        #Get the Q,K,V matrices\n",
    "        Q = self.wq(x)\n",
    "        K = self.wk(x)\n",
    "        V = self.wv(x)\n",
    "        \n",
    "        #Reshape the Matrices so that all 3 contains Heads num and Heads Dim\n",
    "        Q = Q.reshape(B, self.num_query_heads, S, self.head_dim)\n",
    "        K = K.reshape(B, self.num_kv_heads, S, self.head_dim)\n",
    "        V = V.reshape(B, self.num_kv_heads, S, self.head_dim)\n",
    "        \n",
    "        #Reshape K,V  to match Q by interleaving\n",
    "        K = torch.repeat_interleave(K, self.group_size, dim = 1)\n",
    "        V = torch.repeat_interleave(V, self.group_size, dim = 1)\n",
    "        \n",
    "        #Compute the Scaled Dot-Product Attention\n",
    "        denominator = torch.sqrt(torch.tensor(self.head_dim, dtype=Q.dtype))\n",
    "        attn_weights = torch.einsum('bnij, bnkj->bnik', Q, K) / denominator\n",
    "        \n",
    "        #Apply Causal Mask\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones_like(attn_weights), diagonal=1).bool()\n",
    "            attn_weights = attn_weights.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        #Compute the Attention Scores\n",
    "        attn_scores = F.softmax(attn_weights, dim = -1)\n",
    "        out = torch.einsum('bnij,bnjd->bnid', attn_scores, V)\n",
    "        out = out.reshape(B, S, -1)\n",
    "        return self.wo(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ff0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import time\n",
    "\n",
    "# seq_lengths = [64,128,256,512,1024]\n",
    "# B, D = 1, 1024\n",
    "\n",
    "# gqa_times = []\n",
    "# mhsa_times = []\n",
    "\n",
    "# for S in seq_lengths:\n",
    "#     x = torch.randn(B, S, D)\n",
    "    \n",
    "#     attn_gqa = GQA(D_model=D)\n",
    "    \n",
    "#     for _ in range(5):\n",
    "#         _ = attn_gqa(x)\n",
    "        \n",
    "#     start = time.perf_counter()\n",
    "#     for _ in range(10):\n",
    "#         _ = attn_gqa(x)\n",
    "#     gqa_times.append((time.perf_counter() - start) / 10)\n",
    "    \n",
    "#     attn_mhsa = MHSA(D_model=D)\n",
    "    \n",
    "#     for _ in range(5):\n",
    "#         _ = attn_mhsa(x)\n",
    "    \n",
    "#     start = time.perf_counter()\n",
    "#     for _ in range(10):\n",
    "#         _ = attn_mhsa(x)\n",
    "#     mhsa_times.append((time.perf_counter() - start) / 10)\n",
    "    \n",
    "# x = np.arange(len(seq_lengths))\n",
    "# width = 0.35\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (10,6))\n",
    "# ax.bar(x - width/2, gqa_times, width, label='GQA')\n",
    "# ax.bar(x + width/2, mhsa_times, width, label='MHSA')\n",
    "\n",
    "# ax.set_ylabel('Latency (seconds)')\n",
    "# ax.set_xlabel('Sequence Length')\n",
    "# ax.set_title('GQA vs MHSA Latency comparison')\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(seq_lengths)\n",
    "# ax.legend()\n",
    "\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89230519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
